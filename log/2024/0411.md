# Day2

## 超长上下文

### 主要收获：

#### 1、BPT
BPT主要参考下面这两个帖子

**[Blockwise Parallel Transformer for Large Context Models](https://zhuanlan.zhihu.com/p/690822015)**

**[分析transformer模型的参数量、计算量、中间激活、KV cache](https://zhuanlan.zhihu.com/p/624740065)**


首先分析一下Vanilla Transformer Attention和FFN显存占用情况：

**Attention**

    1. 对于Q、K、V，共同的输入x都需要保存，输入shape为[b,s,h]，需要2bsh（假设使用fp16）。
    2. 对于Q和K矩阵乘，需要保存Q和K的激活值，两个张量shape都为[b,s,h]，需要2*2*bsh=4bsh。
    3. 对于softmax，需要保存矩阵乘的结果，Q shape为[b, head_num, s, per_head_hidden_size], K转置的shape为[b, head_num, per_head_hidden_size, s], 所以矩阵乘的最终shape为[b, head_num, s, s]，需要2*bs^2a(其中a是注意力头数量)。
    4. 掩码mask，需要bs^2a。
    5. 计算score x V，需要保存score占用2bs^2a 以及 V占用2bsh。
    6. 投影输出以及drop，需要保存输入2bsh以及dropout掩码，占用bsh。

使用checkpointing技术，最大激活需要O(s^2)。

**FFN**

    1. 第一个线性层，保存输入，2bsh。
    2. 对于激活函数，保存输入，需要2bs*4h，因为第一个线性层从h映射到4h。
    3. 对于第二个线性层，保存输入，需要8bsh，然后第二个线性层从4h映射到h。
    4. 对于dropout，保存掩码mask需要bsh。

使用checkpointing技术，最大激活需要O(8bsh)。


尽管使用FlashAttention以及其他Memory Efficient Attention，可以把softmax进行分块计算，并且是一种精确的自注意力计算方式，大大减少了attention的显存需求，可以把最大激活控制到2bsh（可以参考Flash Attention计算逻辑）。但是随之而来的就是FFN也带来了比较大的挑战，因为需要8bsh。

很自然的想法就是计算Attention的时候，也顺便把FFN也一起计算。
1. 外循环，遍历每个块并计算query。
2. 内循环中，遍历每个块计算key和value，kv对与q一起计算分块注意力。然后使用分块注意力计算前馈网络的输出。

**BPT FFN**

    1. 当迭代FFN的时候，第一个线性层的输入需要2bch。
    2. 对于激活函数，保存输入，需要8bch。
    3. 第二个线性层，保存输入，需要8bch。
    4. 对于dropout，保存掩码mask需要bch。
    5. 存储for循环的输出需要2bsh。

每个BPT层的最大激活是O(2bsh)。

#### 2、Ring Attention
原始BPT存在的问题主要是在内循环中：
1. 每个qi都需要，与所有的key和value进行计算，因此需要从其他节点获取对应的kj和vj，导致计算延迟。
2. 累积的kv块会导致完整的输出会线性增长。


外循环和BPT一样，还是按照q进行分块
内循环，按照节点数进行迭代，每次迭代，从h+1获取一个kv block，然后发送当前的kv block到h-1
* 注意：
    最终所有的节点都会重复计算完softmax要求的分母项目，然后当前节点原始kv块作为分子项，最终的结果就可以求出；
    相对于BPT，内循环需要获取所有节点的kv再计算不一样。

### 主要疑惑：
1、checkpointing技术，具体是怎么实现的？


2、Flash Attention没有解决每一层需要输出存储的问题么？
* Flash Attention优化了注意力计算内存的使用，并不直接解决内循环需要使用所有kv的问题；另外每一层完整存储仍然需要，以供后续层注意力计算使用。